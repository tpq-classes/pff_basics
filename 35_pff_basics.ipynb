{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a396b1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a1eb6",
   "metadata": {},
   "source": [
    "# Python for Finance Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b58ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "&copy; Dr. Yves J. Hilpisch | The Python Quants GmbH\n",
    "\n",
    "http://tpq.io | [training@tpq.io](mailto:trainin@tpq.io) | [@dyjh](http://twitter.com/dyjh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005cbbd",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/tpq-classes/pff_basics.git\n",
    "import sys\n",
    "sys.path.append('pff_basics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import plt\n",
    "np.set_printoptions(suppress=True)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342c353",
   "metadata": {},
   "source": [
    "## `CartPole` Game\n",
    "\n",
    "**Environment & Agents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f75ce",
   "metadata": {},
   "source": [
    "Topics:\n",
    "\n",
    "**Reinforcement Learning**\n",
    "\n",
    "* environment\n",
    "* state\n",
    "* agent\n",
    "* action\n",
    "* step\n",
    "* reward/penalty\n",
    "* objective\n",
    "* policy\n",
    "* episode\n",
    "\n",
    "\n",
    "**Deep Q-Learning**\n",
    "\n",
    "* reward function\n",
    "* action policy\n",
    "* representation\n",
    "* deep neural network\n",
    "* exploration/exploitation\n",
    "* replay & policy update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9bb6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e02531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837842bd",
   "metadata": {},
   "source": [
    "## Memory-Prediction Framework\n",
    "\n",
    "From Wikipedia (see [Memory-Prediction Framework](https://en.wikipedia.org/wiki/Memory-prediction_framework)):\n",
    "\n",
    "> The memory-prediction framework is a theory of brain function created by Jeff Hawkins and described in his 2004 book On Intelligence. This theory concerns the role of the mammalian neocortex and its associations with the hippocampi and the thalamus in matching sensory inputs to stored memory patterns and how this process leads to predictions of what will happen in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd896d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e496031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f334e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21122de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aada7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.1\n",
    "        self.memory = list()\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.trewards = list()\n",
    "        self.averages = list()\n",
    "        self.max_treward = 0\n",
    "        self._create_model()\n",
    "    def _create_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, activation='relu', input_dim=4))\n",
    "        self.model.add(Dense(24, activation='relu'))\n",
    "        self.model.add(Dense(2, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=opt)\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return env.action_space.sample()  # exploration\n",
    "        return np.argmax(self.model.predict(state)[0])  # exploitation\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, next_state, reward, done in batch:\n",
    "            if not done:\n",
    "                reward += self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target = self.model.predict(state)\n",
    "            target[0, action] = reward\n",
    "            self.model.fit(state, target, epochs=1, verbose=False)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    def learn(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = env.reset()\n",
    "            state = np.reshape(state, [1, 4])\n",
    "            for f in range(1, 301):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, trunc, _ = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, 4])\n",
    "                self.memory.append([state, action, next_state, reward, done])\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    self.trewards.append(f)\n",
    "                    av = sum(self.trewards[-20:]) / 20\n",
    "                    self.averages.append(av)\n",
    "                    self.max_treward = max(self.max_treward, f)\n",
    "                    templ = f'episode={e:4d} | treward={f:3d} | '\n",
    "                    templ += f'av={av:5.1f} | max={self.max_treward:3d}'\n",
    "                    print(templ, end='\\r')\n",
    "                    break\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay()\n",
    "        print()\n",
    "    def test(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = env.reset()\n",
    "            state = np.reshape(state, [1, 4])\n",
    "            for f in range(1, 301):\n",
    "                action = np.argmax(self.model.predict(state)[0])  # exploitation\n",
    "                state, reward, done, trunc, _ = env.step(action)\n",
    "                state = np.reshape(state, [1, 4])\n",
    "                if done or f > 299:\n",
    "                    print(f, end=' ')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQLAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time agent.learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9774b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(agent.trewards[:50]) / 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79740d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(agent.trewards[-50:]) / 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.averages);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcea551",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
    "\n",
    "<a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"mailto:training@tpq.io\">training@tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}